# Gemini Service Implementation Guide

## Overview
The Gemini AI service is fully implemented and integrated into your PromptEngine backend. Here's exactly where it's implemented and how it works:

---

## 1. ‚úÖ Gemini Service Class - `backend/gemini_service.py`

### Location
```
backend/
‚îî‚îÄ‚îÄ gemini_service.py (155 lines)
```

### Four Key Methods

#### 1Ô∏è‚É£ `craft_prompt()` - Structured Prompt Engineering
```python
def craft_prompt(self, original_prompt: str, mode: str = "general") -> dict
```
- **Purpose**: Creates structured, mode-specific prompts
- **Returns**: Dictionary with `system_role`, `instruction`, `constraints`, `example`, `response_format`, and `full` prompt
- **Modes Supported**:
  - `ai-dev`: For development & coding tasks
  - `image-gen`: For image generation
  - `chatbot`: For conversational AI
  - `data-analysis`: For data science tasks

**Example**:
```python
gemini_service = GeminiService()
structured_prompt = gemini_service.craft_prompt(
    "Create a Python function for sorting",
    mode="ai-dev"
)
# Returns: {
#   "system_role": "You are Gemini...",
#   "instruction": "Create a Python function...",
#   "constraints": "Provide implementation details...",
#   "example": "When asked to implement...",
#   "response_format": "...",
#   "full": "Complete optimized prompt"
# }
```

---

#### 2Ô∏è‚É£ `optimize_prompt()` - Main Optimization Engine
```python
def optimize_prompt(self, original_prompt: str, mode: str = "ai-dev", options: dict = None) -> str
```
- **Purpose**: Optimizes prompts using Gemini API with fallback support
- **Options**:
  - `include_tests`: Add test cases
  - `add_documentation`: Add documentation
  - `performance_optimization`: Optimize for speed
  - `security_features`: Add security considerations

**How It Works**:
1. **First Attempt**: Uses Gemini API (if GEMINI_API_KEY is set)
2. **Fallback**: Uses rule-based optimization if API fails

**Example**:
```python
optimized = gemini_service.optimize_prompt(
    "Create a function to sort arrays",
    mode="ai-dev",
    options={
        "include_tests": True,
        "add_documentation": True,
        "performance_optimization": True,
        "security_features": False
    }
)
```

---

#### 3Ô∏è‚É£ `generate_quality_scores()` - Quality Analysis
```python
def generate_quality_scores(self, prompt: str) -> dict
```
- **Purpose**: Analyzes prompt quality on 4 dimensions
- **Returns**: Scores for:
  - `clarity`: 0-10 (based on action verbs and sentence structure)
  - `specificity`: 0-10 (based on specific terms)
  - `creativity`: 0-10 (based on punctuation and style)
  - `technical`: 0-10 (based on technical vocabulary)
  - `overall`: 0-10 (average of all scores)

**Example**:
```python
scores = gemini_service.generate_quality_scores(
    "Create a comprehensive Python sorting function with proper error handling"
)
# Returns: {
#   "clarity": 8.5,
#   "specificity": 7.2,
#   "creativity": 6.0,
#   "technical": 8.9,
#   "overall": 7.65
# }
```

---

#### 4Ô∏è‚É£ `generate_assistant_response()` - AI Chat
```python
def generate_assistant_response(self, user_message: str, prompt_context: str = None) -> str
```
- **Purpose**: Generates AI assistant responses for user questions
- **Uses**: Gemini API with fallback to preset responses

**Example**:
```python
response = gemini_service.generate_assistant_response(
    "How can I improve my prompt?",
    prompt_context="ai-dev"
)
# Returns: Helpful response about prompt optimization
```

---

## 2. ‚úÖ Integration in API Endpoints - `backend/main.py`

### Where Gemini is Used

#### Endpoint 1: `POST /optimize` (Line 75)
```python
@app.post("/optimize", response_model=schemas.OptimizePromptResponse)
def optimize_prompt(request: schemas.OptimizePromptRequest, db: Session = Depends(database.get_db)):
    # Step 1: Create prompt record in database
    prompt_record = models.Prompt(original=request.original_prompt, mode=request.mode)
    
    # Step 2: Call Gemini service to optimize
    optimized_prompt = gemini_service.optimize_prompt(
        request.original_prompt,
        request.mode,
        options={...}
    )
    
    # Step 3: Generate quality scores using Gemini
    scores = gemini_service.generate_quality_scores(optimized_prompt)
    
    # Step 4: Save everything to database
    # Return optimized prompt + scores to frontend
```

#### Endpoint 2: `POST /analyze` (Line 125)
```python
@app.post("/analyze", response_model=schemas.AnalyzePromptResponse)
def analyze_prompt(request: schemas.OptimizePromptRequest, db: Session = Depends(database.get_db)):
    # Analyzes prompt using Gemini quality scores
    scores = gemini_service.generate_quality_scores(request.original_prompt)
    
    # Returns detailed breakdown of prompt quality
```

#### Endpoint 3: `POST /assistant` (Line 190)
```python
@app.post("/assistant", response_model=schemas.AssistantMessageResponse)
def send_message(request: schemas.AssistantMessageRequest, db: Session = Depends(database.get_db)):
    # Uses Gemini to generate assistant response
    response = gemini_service.generate_assistant_response(
        request.user_message,
        request.prompt_context
    )
    
    # Saves conversation to database
```

---

## 3. ‚úÖ Configuration - `backend/.env`

Your `.env` file has the Gemini API key:

```env
# Gemini API
GEMINI_API_KEY=AIzaSyAbPkBjpyBWzceE2xTMdPcIrRxXQ51Gu5Y
```

This key is loaded in `backend/config.py`:
```python
import os
from dotenv import load_dotenv

load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
```

---

## 4. ‚úÖ Frontend Integration - `frontend/main.js`

Your frontend calls the Gemini-powered backend:

```javascript
// Line 1-30: API Client Configuration
const API_BASE_URL = 'http://127.0.0.1:8000';

const apiClient = {
    async post(endpoint, data) {
        const response = await fetch(`${API_BASE_URL}${endpoint}`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(data)
        });
        return await response.json();
    }
};

// Line 150-170: optimizePrompt() calls backend
async function optimizePrompt() {
    const response = await apiClient.post('/optimize', {
        original_prompt: userPrompt,
        mode: currentMode,
        include_tests: true,
        add_documentation: true,
        performance_optimization: true,
        security_features: false
    });
    // Gemini optimizes on backend, returns result
    displayOptimizedPrompt(response.optimized_prompt);
    displayQualityScores(response.quality_scores);
}

// Line 200-220: analyzePromptQuality() calls backend
async function analyzePromptQuality() {
    const response = await apiClient.post('/analyze', {
        original_prompt: userPrompt,
        mode: currentMode
    });
    // Gemini analyzes quality on backend
    displayAnalysis(response);
}

// Line 260-280: sendAssistantMessage() calls backend
async function sendAssistantMessage() {
    const response = await apiClient.post('/assistant', {
        user_message: chatInput,
        prompt_context: currentMode
    });
    // Gemini generates AI response on backend
    displayChatMessage(response);
}
```

---

## 5. ‚úÖ Complete Request/Response Flow

### Example: User Optimizes a Prompt

```
Frontend (index.html)
    ‚Üì
    User enters: "Create a sorting function"
    Clicks "Optimize Prompt"
    ‚Üì
Frontend (main.js)
    ‚Üì
    POST /optimize endpoint
    {
        "original_prompt": "Create a sorting function",
        "mode": "ai-dev",
        "include_tests": true,
        ...
    }
    ‚Üì
Backend (main.py - /optimize endpoint)
    ‚Üì
    Create database record (prompts table)
    ‚Üì
    Call gemini_service.optimize_prompt()
    ‚Üì
GeminiService (gemini_service.py)
    ‚Üì
    Try: Use Gemini API (if GEMINI_API_KEY set)
    Fallback: Use rule-based optimization
    ‚Üì
    Returns: "Create a comprehensive Python sorting function..."
    ‚Üì
Backend (continues)
    ‚Üì
    Call gemini_service.generate_quality_scores()
    ‚Üì
    Returns: { clarity: 8.5, specificity: 7.2, ... }
    ‚Üì
    Save quality scores to database (quality_scores table)
    Save optimization history (optimization_history table)
    ‚Üì
    Return response to frontend:
    {
        "optimized_prompt": "Create a comprehensive...",
        "quality_scores": {
            "clarity": 8.5,
            "specificity": 7.2,
            "creativity": 6.0,
            "technical": 8.9,
            "overall": 7.65
        },
        "improvement_percentage": 15.5
    }
    ‚Üì
Frontend (main.js)
    ‚Üì
    Display optimized prompt
    Display quality scores with visual indicators
    Show improvement percentage
```

---

## 6. Database Integration

### Tables That Store Gemini Results

```sql
-- 1. Original and optimized prompts
CREATE TABLE prompts (
    id INTEGER PRIMARY KEY,
    original TEXT,          -- User's original prompt
    optimized TEXT,         -- Gemini optimized prompt
    mode VARCHAR(50),       -- Mode used (ai-dev, etc)
    created_at DATETIME,
    updated_at DATETIME
);

-- 2. Quality scores from Gemini analysis
CREATE TABLE quality_scores (
    id INTEGER PRIMARY KEY,
    prompt_id INTEGER,
    clarity FLOAT,          -- Gemini quality metric
    specificity FLOAT,      -- Gemini quality metric
    creativity FLOAT,       -- Gemini quality metric
    technical FLOAT,        -- Gemini quality metric
    overall FLOAT,          -- Average of all metrics
    created_at DATETIME
);

-- 3. Optimization history tracking
CREATE TABLE optimization_history (
    id INTEGER PRIMARY KEY,
    prompt_id INTEGER,
    original_prompt TEXT,
    optimized_prompt TEXT,
    mode VARCHAR(50),
    model VARCHAR(50),      -- "gemini-pro"
    improvement_percentage FLOAT,
    created_at DATETIME
);

-- 4. AI Assistant conversation
CREATE TABLE assistant_messages (
    id INTEGER PRIMARY KEY,
    user_message TEXT,
    assistant_response TEXT,  -- Gemini generated response
    prompt_context VARCHAR(50),
    created_at DATETIME
);
```

---

## 7. How to Test Gemini Service

### 1. Start Backend
```powershell
cd backend
python main.py
```

### 2. Access Swagger UI
Visit: `http://127.0.0.1:8000/docs`

### 3. Test `/optimize` Endpoint
```json
{
  "original_prompt": "Create a function to find the largest number",
  "mode": "ai-dev",
  "include_tests": true,
  "add_documentation": true,
  "performance_optimization": true,
  "security_features": false
}
```

### 4. Expected Response
```json
{
  "optimized_prompt": "Create a comprehensive Python function...",
  "quality_scores": {
    "clarity": 8.5,
    "specificity": 7.8,
    "creativity": 6.2,
    "technical": 8.9,
    "overall": 7.85
  },
  "improvement_percentage": 18.5,
  "mode": "ai-dev",
  "created_at": "2025-11-16T18:46:25.123456"
}
```

---

## 8. Gemini Service Features Summary

| Feature | Status | Location |
|---------|--------|----------|
| Prompt Crafting | ‚úÖ Implemented | `gemini_service.py` line 12 |
| Prompt Optimization | ‚úÖ Implemented | `gemini_service.py` line 46 |
| Quality Scoring | ‚úÖ Implemented | `gemini_service.py` line 106 |
| AI Chat Responses | ‚úÖ Implemented | `gemini_service.py` line 131 |
| API Integration | ‚úÖ Implemented | `/optimize`, `/analyze`, `/assistant` endpoints |
| Database Storage | ‚úÖ Implemented | 6 SQLAlchemy models |
| Frontend Connection | ‚úÖ Implemented | `main.js` API calls with fallback |
| Mode Support | ‚úÖ Implemented | ai-dev, image-gen, chatbot, data-analysis |
| Error Handling | ‚úÖ Implemented | Try-catch with fallback logic |
| Logging | ‚úÖ Implemented | Console output on backend startup |

---

## 9. Next Steps

### To Use Gemini Service Now:
1. ‚úÖ Backend is running: `http://127.0.0.1:8000`
2. ‚úÖ Gemini is configured with API key in `.env`
3. ‚úÖ Frontend is ready at: `http://127.0.0.1:8080`

### Test It:
1. Open frontend in browser
2. Enter a prompt in the text area
3. Click "Optimize Prompt" button
4. Wait for Gemini to process and return results
5. View optimized prompt + quality scores

### Monitor Gemini Activity:
- Backend console shows: `üìù Optimize request received - Mode: ai-dev`
- Check database tables for stored results

---

## 10. Troubleshooting

### If Gemini API fails:
- Check `GEMINI_API_KEY` in `.env` is correct
- Service automatically falls back to rule-based optimization
- Check backend logs for error messages

### If requests are slow:
- First Gemini request might be slower
- API might have rate limits
- Fallback mode is faster but less intelligent

### If database doesn't save results:
- Verify MySQL is running
- Check `DATABASE_URL` in `.env`
- Review backend logs for database errors

---

**Summary**: ‚úÖ Gemini service is **FULLY IMPLEMENTED** and ready to use!
All 4 methods are active, integrated in 3 API endpoints, connected to frontend, and backed by MySQL database.
